{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edbb6db-f6ee-420d-ae2e-e10cb20f94eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from random import randint\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "# plt.style.use('seaborn-darkgrid')\n",
    "import seaborn as sns\n",
    "# sns.set_style(\"white\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from skimage.transform import resize\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout, BatchNormalization,UpSampling2D,Concatenate\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f942592-acc2-437f-a5d0-ef256bd73f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "os.makedirs(\"train\", exist_ok=True)\n",
    "\n",
    "path_to_zip_file = \"/kaggle/input/tgs-salt-identification-challenge/train.zip\"\n",
    "directory_to_extract_to = 'train/'\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d99d33f-baea-41be-a0ed-7dc00e16a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the directories to find the data we will need (online directories). You can always change these to run locally with a different directory structure.\n",
    "TRAIN_IMAGE_DIR = 'kaggle/working/train/images/'\n",
    "TRAIN_MASK_DIR = 'kaggle/working/train/masks/'\n",
    "MAIN_DIR = '../kaggle/input/tgs-salt-identification-challenge/'\n",
    "\n",
    "# Making a two-column data frame for the training data\n",
    "train_df = pd.read_csv(\"/kaggle/input/tgs-salt-identification-challenge/train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"/kaggle/input/tgs-salt-identification-challenge/depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a638b1c4-8dab-4c45-8691-7b51ebb19fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd58eab-3292-41df-a297-704ea9288eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_train = len(train_df)\n",
    "print(\"The number of rows in the training dataframe is\",length_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2f1cd6-9292-4ff9-b3be-6b41aebf6337",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE_DIR = '/kaggle/working/train/images/'\n",
    "TRAIN_MASK_DIR = '/kaggle/working/train/masks/'\n",
    "\n",
    "# Listing the png files\n",
    "train_img_list = glob.glob(TRAIN_IMAGE_DIR+'*.png')\n",
    "mask_img_list = glob.glob(TRAIN_IMAGE_DIR+'*.png')\n",
    "\n",
    "# Sizes of every png file\n",
    "train_img_sizes = [Image.open(f, 'r').size for f in train_img_list]\n",
    "mask_img_sizes = [Image.open(f, 'r').size for f in mask_img_list]\n",
    "\n",
    "# Printing the results of looking at the minimum and maximum image sizes\n",
    "print(\"The minimum train image size is\", min(train_img_sizes), \"and the max is\",max(train_img_sizes))\n",
    "print(\"The minimum training mask image size is\", min(mask_img_sizes),\"and the max is\",max(mask_img_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95908ea-122c-40cd-8f1b-00b5349e06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size_ori = 101\n",
    "img_size_target = 128\n",
    "\n",
    "def upsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "\n",
    "# When downsampling images, normally some kind of anti-aliasing should be used. \n",
    "def downsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab85008d-22f2-4fc4-9f03-555fa6dc4939",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"images\"] = [np.array(load_img(\"/kaggle/working/train/images/{}.png\".format(idx), color_mode=\"grayscale\")) / 255 for idx in tqdm_notebook(train_df.index)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03afc81e-6605-4e69-bfa3-a82ffe38ed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"masks\"] = [np.array(load_img(\"/kaggle/working/train/masks/{}.png\".format(idx), color_mode=\"grayscale\")) / 255 for idx in tqdm_notebook(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18db5ee-30b9-4a45-abad-b7093b6a1a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6185d7a-0367-4f0e-adaa-c147227eda32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9032d1-ef84-4f43-ae65-6a68dd8136fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"coverage\"] = train_df[\"coverage\"].values/(101**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e68356b-5f78-4ff3-8097-26754a8c8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"coverage\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4733e93c-c328-4321-8a5c-302a55895caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "\n",
    "        \n",
    "train_df[\"coverage\"] = train_df[\"coverage\"]/train_df[\"coverage\"].max()\n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47f6268-1867-4293-b1b8-740baf811327",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00930fa-a60f-4e7f-af50-84d1198bf198",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n",
    "    train_df.index.values,\n",
    "    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    train_df.coverage.values,\n",
    "    train_df.z.values,\n",
    "    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a396c50e-cc9d-4606-90c9-23a1bffa21e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_layer, start_neurons):\n",
    "    # 128 -> 64\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    pool1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    # 64 -> 32\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    pool2 = Dropout(0.5)(pool2)\n",
    "\n",
    "    # 32 -> 16\n",
    "    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    pool3 = Dropout(0.5)(pool3)\n",
    "\n",
    "    # 16 -> 8\n",
    "    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n",
    "    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(0.5)(pool4)\n",
    "\n",
    "    # Middle\n",
    "    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n",
    "    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(convm)\n",
    "\n",
    "    # 8 -> 16\n",
    "    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv4 = concatenate([deconv4, conv4])\n",
    "    uconv4 = Dropout(0.5)(uconv4)\n",
    "    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n",
    "    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n",
    "\n",
    "    # 16 -> 32\n",
    "    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n",
    "    uconv3 = concatenate([deconv3, conv3])\n",
    "    uconv3 = Dropout(0.5)(uconv3)\n",
    "    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n",
    "    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n",
    "\n",
    "    # 32 -> 64\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    uconv2 = Dropout(0.5)(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "\n",
    "    # 64 -> 128\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    uconv1 = Dropout(0.5)(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "\n",
    "    ucov1 = Dropout(0.5)(uconv1)\n",
    "    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n",
    "    \n",
    "    return output_layer\n",
    "\n",
    "input_layer = Input((img_size_target, img_size_target, 1))\n",
    "output_layer = build_model(input_layer, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1b9dbc-cae6-4eef-bf6a-e41cf1de9b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(m, dim, acti, bn, res, do=0):\n",
    "    n = Conv2D(dim, 3, activation=acti, padding='same')(m)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    n = Dropout(do)(n) if do else n\n",
    "    n = Conv2D(dim, 3, activation=acti, padding='same')(n)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    return Concatenate()([m, n]) if res else n\n",
    "\n",
    "def level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n",
    "    if depth > 0:\n",
    "        n = conv_block(m, dim, acti, bn, res)\n",
    "        m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
    "        m = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n",
    "        if up:\n",
    "            m = UpSampling2D()(m)\n",
    "            m = Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
    "        else:\n",
    "            m = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n",
    "        n = Concatenate()([n, m])\n",
    "        m = conv_block(n, dim, acti, bn, res)\n",
    "    else:\n",
    "        m = conv_block(m, dim, acti, bn, res, do)\n",
    "    return m\n",
    "\n",
    "def UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n",
    "         dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n",
    "    i = Input(shape=img_shape)\n",
    "    o = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n",
    "    o = Conv2D(out_ch, 1, activation='sigmoid')(o)\n",
    "    return Model(inputs=i, outputs=o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df4ecc-71b9-4d4a-83ec-7e5cc3784a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet((img_size_target,img_size_target,1),start_ch=16,depth=5,batchnorm=True)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21665fae-f238-4cc9-98a7-1738e5d9fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n",
    "y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f28e8-e1fa-4d7b-8cff-9f4b6ff4975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 10, figsize=(15,3))\n",
    "for i in range(10):\n",
    "    axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n",
    "    axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n",
    "    axs[1][i].imshow(x_train[int(len(x_train)/2 + i)].squeeze(), cmap=\"Greys\")\n",
    "    axs[1][i].imshow(y_train[int(len(y_train)/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\n",
    "fig.suptitle(\"Top row: original images, bottom row: augmented images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae704b80-e42b-4df8-b04e-441aa663b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=10, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(\"model.keras\", save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.000001, verbose=1)\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=[x_valid, y_valid], \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint, reduce_lr])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
