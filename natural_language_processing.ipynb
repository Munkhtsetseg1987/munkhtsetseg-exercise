{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f5ba89-b90f-4fb5-aa5e-36505b0d9125",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar zxf aclImdb_v1.tar.gz\n",
    "!rm -rf aclImdb/train/unsup\n",
    "!cat aclImdb/README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8fd7a-6b88-49e1-aab8-d74aa9fa899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target\n",
    "\n",
    "# ラベルの0,1と意味の対応の表示\n",
    "print(train_review.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa4e116-a93a-4ddf-82d8-f4b70faea5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x : {}\".format(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511fa295-af8c-4585-b542-0b3f41b78538",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset = \\\n",
    "  [\"This movie is very good.\",\n",
    "  \"This film is a good\",\n",
    "  \"Very bad. Very, very bad.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb3c0ad-b849-41f7-a387-cfb204d843c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 1-grim\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df1 = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# 2-grim\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df2 = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"1-gram\")\n",
    "display(df1)\n",
    "print(\"2-grim\")\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2b531-dc74-423e-a2cb-5165b02046a5",
   "metadata": {},
   "source": [
    "[Problem 1] Scratch implementation of BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62016063-be00-4eb5-9660-6cedaddd7921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "n_samples = 25000\n",
    "idf = np.log(n_samples/np.arange(1,n_samples))\n",
    "plt.title(\"IDF\")\n",
    "plt.xlabel(\"df(t)\")\n",
    "plt.ylabel(\"IDF\")\n",
    "plt.plot(idf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fc246c-28bc-455a-a7d7-c074ee5abcec",
   "metadata": {},
   "source": [
    "[Problem 2] Calculation of TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c2c7a4-0a96-4eb5-ae3e-8f17aa0580bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stop_words = nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(\"stop word : {}\".format(stop_words)) # 'i', 'me', 'my', ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49db7b56-d066-446c-9fef-7d7a86cc0993",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stop_words, max_features = 5000)\n",
    "bow_train = (vectorizer.fit_transform(x_train)).toarray()\n",
    "bow_test = (vectorizer.fit_transform(x_test)).toarray()\n",
    "#print(bow_train)\n",
    "df_train = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names_out())\n",
    "df_test = pd.DataFrame(bow_test, columns=vectorizer.get_feature_names_out())\n",
    "display(df_train.head(20))\n",
    "display(df_test.head(20))\n",
    "print(\"df train shape: {}\".format(df_train.shape))\n",
    "print(\"df test shape: {}\".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26cd5bc-bbf7-4158-9907-b706ab0b0e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df train shape: (25000, 5000)\n",
    "df test shape: (25000, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538bbe3e-9612-40c3-becc-5b1991975bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm='l2',max_features = 5000,stop_words=stop_words)\n",
    "X_train = vectorizer.fit_transform(x_train)\n",
    "X_test = vectorizer.transform(x_test)\n",
    "#print(\"Feature names: {}\".format(vectorizer.get_feature_names_out()))\n",
    "print(\"train shape: {}\".format(X_train.shape))\n",
    "print(\"tets shape: {}\".format(X_test.shape))\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f164d48-182e-4ee8-be2f-d0fb13e26d6e",
   "metadata": {},
   "source": [
    "[Problem 3] Learning using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ae9fc-337b-40db-9620-7a2d35db852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear', random_state=0, verbose=True)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387dc146-2033-43e4-a0f5-c89e47d32089",
   "metadata": {},
   "source": [
    "[Problem 4] Scratch implementation of TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2533ce51-29fe-4c74-bdd7-2cead529e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF(object):\n",
    "\n",
    "    def __init__(self, corpus):        \n",
    "        self.corpus = corpus      \n",
    "\n",
    "    def __normalize_corpus(self, d):\n",
    "        stop_words = nltk.corpus.stopwords.words('english')\n",
    "        d = re.sub(r'[^a-zA-Z0-9\\s]', '', d, re.I|re.A)\n",
    "        d = d.lower().strip()\n",
    "        tks = nltk.word_tokenize(d)\n",
    "        f_tks = [t for t in tks if t not in stop_words]\n",
    "        return ' '.join(f_tks)\n",
    "\n",
    "    def preprocessing_text(self):\n",
    "        n_c = np.vectorize(self.__normalize_corpus)\n",
    "        self.norm_corpus = n_c(self.corpus)\n",
    "\n",
    "    def tf(self):\n",
    "        words_array = [doc.split() for doc in self.norm_corpus]\n",
    "        words = list(set([word for words in words_array for word in words]))\n",
    "        features_dict = {w:0 for w in words}\n",
    "        tf = []\n",
    "        for doc in self.norm_corpus:\n",
    "            bowf_doc = Counter(doc.split())\n",
    "            all_f = Counter(features_dict)\n",
    "            bowf_doc.update(all_f)\n",
    "            tf.append(bowf_doc)\n",
    "        return pd.DataFrame(tf)\n",
    "\n",
    "    def df(self, tf):\n",
    "        features_names = list(tf.columns)\n",
    "        df = np.diff(sp.csc_matrix(tf, copy=True).indptr)\n",
    "        df = 1 + df\n",
    "        return df\n",
    "        \n",
    "    def idf(self, df):\n",
    "        N = 1 + len(self.norm_corpus)\n",
    "        idf = (1.0 + np.log(float(N) / df)) \n",
    "        idf_d = sp.spdiags(idf, diags= 0, m=len(df), n= len(df)).todense()      \n",
    "        return idf, idf_d\n",
    "\n",
    "    def tfidf(self, tf, idf):        \n",
    "        tf = np.array(tf, dtype='float64')\n",
    "        tfidf = tf * idf\n",
    "        norms = norm(tfidf , axis=1)\n",
    "        return (tfidf / norms[:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a4272-7e18-47e9-ad9e-4aa949c42edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b200c0b5-5633-462c-88ed-5804d6bef58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "mini_dataset2 = \\\n",
    "    [\"This movie is SOOOO funny!!!\",\n",
    "    \"What a movie! I never\",\n",
    "    \"best movie ever!!!!! this movie\"]\n",
    "    \n",
    "    \n",
    "tfidf_scratch = TFIDF(mini_dataset2)\n",
    "tfidf_scratch.preprocessing_text()\n",
    "\n",
    "tf = tfidf_scratch.tf()\n",
    "print(\"TF:\\n\",tf)\n",
    "\n",
    "df = tfidf_scratch.df(tf)\n",
    "print(\"df\\n\",df)\n",
    "\n",
    "idf, idf_2 = tfidf_scratch.idf(df)\n",
    "print(\"IDF:\\n\",idf)\n",
    "\n",
    "tfidf = tfidf_scratch.tfidf(tf,idf)\n",
    "print(\"TF-IDF: \\n\",tfidf)\n",
    "\n",
    "df = pd.DataFrame(np.round(tfidf,2),columns = list(tf.columns))\n",
    "display(df)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4efc08-082a-466b-9d36-0faeff165877",
   "metadata": {},
   "source": [
    "[Problem 5] Corpus preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11471e8-59a5-49f4-be13-a936c5d78007",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stop_words, max_features = 5000)\n",
    "bow_train = (vectorizer.fit_transform(x_train)).toarray()\n",
    "sentences = vectorizer.get_feature_names_out()\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caee605d-d613-4eb6-92ac-9266bae0e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(min_count=1, vector_size=10) # 次元数を10に設定\n",
    "model.build_vocab(sentences) # 準備\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs) # 学習\n",
    "\n",
    "print(\"Vocabulary list : {}\".format(model.wv.key_to_index.keys()))\n",
    "\n",
    "for vocab in model.wv.key_to_index.keys():\n",
    "  print(\"{} vector of : \\n{}\".format(vocab, model.wv[vocab]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
